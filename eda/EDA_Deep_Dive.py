# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.19.1
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %% [markdown]
# # EDA Deep Dive: MVRV, Polymarket Sentiment & Regime Analysis**Purpose:** This notebook extends `EDA.ipynb` with targeted analyses that directlymotivate feature engineering for the Bitcoin DCA weight model.**Sections:**1. **MVRV Z-Score Deep Dive** — Zone classification, forward returns, gradient & acceleration, mean reversion2. **BTC-Specific Polymarket Sentiment** — Filtered BTC markets, daily sentiment, event-window analysis3. **Volatility & Regime Analysis** — Regime detection, feature behaviour by regime, net exchange flows4. **Additional Signal Exploration** — Fee market, exchange supply ratio, halving cycle5. **Polymarket Odds Dynamics** — Probability velocity, convergence patterns in BTC markets6. **Network Growth & Address Activity** — Active addresses, balance growth, on-chain engagement7. **Derived Metrics** — NUPL, volume turnover, hash ribbon / miner capitulation8. **Time-Series Diagnostics** — Autocorrelation structure, rolling correlation stability9. **Feature Importance Ranking** — Synthesis of which signals best predict accumulation opportunity> All features are explored with a 1-day lag to match production constraints (no look-ahead bias).

# %% [markdown]
# ---## 0 · Setup & Data Loading

# %%
from contextlib import contextmanagerfrom pathlib import Pathfrom typing import Optionalimport warningsimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesimport numpy as npimport polars as plimport pandas as pdimport seaborn as snsfrom scipy import statswarnings.filterwarnings('ignore', category=FutureWarning)sns.set_theme(style="whitegrid", palette="muted", font_scale=1.05)plt.rcParams.update({'figure.dpi': 130, 'savefig.dpi': 150, 'figure.facecolor': 'white'})# --- Path resolution ---cwd = Path.cwd().resolve()PROJECT_ROOT = Path('/home/jovyan/work/bitcoin-analytics-capstone-template')if not PROJECT_ROOT.exists():    for c in [cwd, cwd.parent, cwd.parent.parent]:        if (c / 'data').exists():            PROJECT_ROOT = c; breakEDA_DIR      = PROJECT_ROOT / 'eda'PLOTS_DIR    = EDA_DIR / 'plots' / 'deep_dive'DATA_DIR     = PROJECT_ROOT / 'data'COINMETRICS_PATH = DATA_DIR / 'Coin Metrics' / 'coinmetrics_btc.csv'POLYMARKET_DIR   = DATA_DIR / 'Polymarket'PLOTS_DIR.mkdir(parents=True, exist_ok=True)print(f'PROJECT_ROOT : {PROJECT_ROOT}')print(f'BTC CSV exists: {COINMETRICS_PATH.exists()}')print(f'Poly dir exists: {POLYMARKET_DIR.exists()}')

# %%
# --- Helpers ---def safe_to_datetime(expr: pl.Expr) -> pl.Expr:    return expr.cast(pl.Utf8).str.to_datetime(strict=False)def to_date_if_present(df: pl.DataFrame, candidates: list[str]) -> pl.DataFrame:    existing = [c for c in candidates if c in df.columns]    if not existing: return df    return df.with_columns([safe_to_datetime(pl.col(c)).alias(c) for c in existing])# --- Load BTC ---btc_df = (pl.scan_csv(COINMETRICS_PATH, infer_schema_length=10000)          .with_columns(safe_to_datetime(pl.col('time')).alias('time'))          .collect())print(f'✓ BTC: {len(btc_df):,} rows × {len(btc_df.columns)} cols')# --- Load Polymarket (markets only — memory efficient) ---dt_cands = ['timestamp','time','created_at','end_date','first_trade','last_trade',            'first_market_start','last_market_end']poly_data = {}for name, fname in [('markets', 'finance_politics_markets.parquet'),                     ('odds', 'finance_politics_odds_history.parquet'),                     ('summary', 'finance_politics_summary.parquet'),                     ('tokens', 'finance_politics_tokens.parquet'),                     ('event_stats', 'finance_politics_event_stats.parquet')]:    path = POLYMARKET_DIR / fname    if path.exists():        df = pl.scan_parquet(path).collect()        df = to_date_if_present(df, [c for c in dt_cands if c in df.columns])        poly_data[name] = df        print(f'✓ {name}: {len(df):,} rows')# Skip trades (6.5 GB) unless specifically neededtrades_path = POLYMARKET_DIR / 'finance_politics_trades.parquet'print(f'Trades file exists: {trades_path.exists()} (skipped for memory — loaded on demand)')

# %% [markdown]
# ---# 1 · MVRV Z-Score Deep DiveThe `example_1` model gives MVRV a **64% signal weight** — making it the dominant feature. This sectionvalidates whether that emphasis is justified by the data.**Key questions:**- What does the MVRV Z-score distribution look like historically?- Do different MVRV "zones" actually predict different forward BTC returns?- Does the gradient (trend direction) or acceleration (momentum) add predictive value?- How reliably does MVRV mean-revert, and how fast?

# %% [markdown]
# ### 1.1 · MVRV Z-Score Construction

# %%
# Compute MVRV Z-Score (365-day rolling window, matching example_1)btc_pd = btc_df.to_pandas()btc_pd['time'] = pd.to_datetime(btc_pd['time'])btc_pd = btc_pd.sort_values('time').set_index('time')# Filter to rows where MVRV existsmvrv_col = 'CapMVRVCur'btc_pd = btc_pd.dropna(subset=[mvrv_col, 'PriceUSD'])print(f'Rows with valid MVRV + Price: {len(btc_pd):,}')print(f'Date range: {btc_pd.index.min().date()} → {btc_pd.index.max().date()}')# Z-score: (x - rolling_mean) / rolling_stdROLLING_WINDOW = 365btc_pd['mvrv_zscore'] = (    (btc_pd[mvrv_col] - btc_pd[mvrv_col].rolling(ROLLING_WINDOW, min_periods=90).mean())    / btc_pd[mvrv_col].rolling(ROLLING_WINDOW, min_periods=90).std()).clip(-4, 4)# Gradient (30-day EMA-smoothed diff)btc_pd['mvrv_gradient'] = np.tanh(    btc_pd['mvrv_zscore'].diff(30).ewm(span=30).mean() * 2)# Acceleration (14-day second derivative)btc_pd['mvrv_acceleration'] = np.tanh(    btc_pd['mvrv_gradient'].diff(14).ewm(span=14).mean() * 3)# Daily returns and forward returnsbtc_pd['daily_ret'] = btc_pd['PriceUSD'].pct_change()for horizon in [7, 14, 30, 60, 90]:    btc_pd[f'fwd_ret_{horizon}d'] = btc_pd['PriceUSD'].pct_change(horizon).shift(-horizon)btc_pd['mvrv_zscore'].describe()

# %% [markdown]
# ### 1.2 · MVRV Z-Score Distribution & Zone Classification

# %%
# Zone classification (matching example_1 thresholds)def classify_zone(z):    if z < -2.0: return 'Deep Value'    elif z < -1.0: return 'Value'    elif z < 1.5: return 'Neutral'    elif z < 2.5: return 'Caution'    else: return 'Danger'zone_order = ['Deep Value', 'Value', 'Neutral', 'Caution', 'Danger']zone_colors = {'Deep Value': '#1a9850', 'Value': '#91cf60', 'Neutral': '#fee08b',               'Caution': '#fc8d59', 'Danger': '#d73027'}valid = btc_pd.dropna(subset=['mvrv_zscore']).copy()valid['zone'] = valid['mvrv_zscore'].apply(classify_zone)# Distribution plotfig, axes = plt.subplots(1, 2, figsize=(14, 5))# Histogramaxes[0].hist(valid['mvrv_zscore'], bins=80, color='steelblue', alpha=0.7, edgecolor='white')for thresh, label, ls in [(-2, 'Deep Value', '--'), (-1, 'Value', ':'),                            (1.5, 'Caution', ':'), (2.5, 'Danger', '--')]:    axes[0].axvline(thresh, color='red', linestyle=ls, alpha=0.7, label=f'{label} ({thresh})')axes[0].set_xlabel('MVRV Z-Score')axes[0].set_ylabel('Count')axes[0].set_title('Distribution of MVRV Z-Score (365-day rolling)')axes[0].legend(fontsize=8)# Zone frequency piezone_counts = valid['zone'].value_counts()zone_counts = zone_counts.reindex(zone_order).dropna()axes[1].pie(zone_counts, labels=zone_counts.index, autopct='%1.1f%%',            colors=[zone_colors[z] for z in zone_counts.index], startangle=90)axes[1].set_title('Time Spent in Each MVRV Zone')plt.tight_layout()plt.savefig(PLOTS_DIR / 'mvrv_zscore_distribution.png', dpi=150, bbox_inches='tight')plt.show()# Print zone summaryprint(f'\nZone frequency (days):')for z in zone_order:    n = (valid['zone'] == z).sum()    pct = n / len(valid) * 100    print(f'  {z:12s}: {n:5d} days ({pct:5.1f}%)')

# %% [markdown]
# ### 1.3 · Forward Returns by MVRV Zone

# %%
# KEY ANALYSIS: Do different zones predict different forward returns?fig, axes = plt.subplots(1, 3, figsize=(16, 5))for ax, horizon in zip(axes, [7, 30, 90]):    col = f'fwd_ret_{horizon}d'    data = valid.dropna(subset=[col])        box_data = [data[data['zone'] == z][col].values * 100 for z in zone_order]    bp = ax.boxplot(box_data, labels=zone_order, patch_artist=True, showmeans=True,                    meanprops=dict(marker='D', markerfacecolor='black', markersize=5),                    medianprops=dict(color='black', linewidth=2))        for patch, z in zip(bp['boxes'], zone_order):        patch.set_facecolor(zone_colors[z])        patch.set_alpha(0.7)        ax.set_ylabel('Forward Return (%)')    ax.set_title(f'{horizon}-Day Forward Return by MVRV Zone')    ax.axhline(0, color='gray', linewidth=0.5)    ax.tick_params(axis='x', rotation=30)plt.suptitle('Do MVRV Zones Predict BTC Forward Returns?', fontsize=14, y=1.02)plt.tight_layout()plt.savefig(PLOTS_DIR / 'mvrv_forward_returns_by_zone.png', dpi=150, bbox_inches='tight')plt.show()# Statistical test: Kruskal-Wallis across zones for 30d returnscol = 'fwd_ret_30d'groups = [valid[valid['zone'] == z][col].dropna().values for z in zone_order]groups = [g for g in groups if len(g) > 5]if len(groups) >= 2:    stat, pval = stats.kruskal(*groups)    print(f'Kruskal-Wallis test (30d forward returns across zones):')    print(f'  H-statistic = {stat:.2f}, p-value = {pval:.2e}')    print(f'  {"✅ Significant" if pval < 0.05 else "❌ Not significant"} at α=0.05')    # Mean forward returns per zoneprint(f'\nMean forward returns by zone:')print(f'{"Zone":>12s} {"7d":>8s} {"30d":>8s} {"90d":>8s}')for z in zone_order:    subset = valid[valid['zone'] == z]    r7  = subset['fwd_ret_7d'].mean() * 100    r30 = subset['fwd_ret_30d'].mean() * 100    r90 = subset['fwd_ret_90d'].mean() * 100    print(f'{z:>12s} {r7:>+7.2f}% {r30:>+7.2f}% {r90:>+7.2f}%')

# %% [markdown]
# ### 1.4 · MVRV Z-Score Over Time with Price

# %%
fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,                        gridspec_kw={'height_ratios': [2, 1.5, 1]})# Price with zone shadingax = axes[0]ax.semilogy(valid.index, valid['PriceUSD'], color='black', linewidth=0.8)for z in zone_order:    mask = valid['zone'] == z    if mask.any():        ax.fill_between(valid.index, valid['PriceUSD'].min(), valid['PriceUSD'].max(),                        where=mask, alpha=0.15, color=zone_colors[z], label=z)ax.set_ylabel('BTC Price (log)')ax.set_title('BTC Price with MVRV Zone Overlay')ax.legend(fontsize=8, ncol=5, loc='upper left')# Z-scoreax = axes[1]ax.plot(valid.index, valid['mvrv_zscore'], color='steelblue', linewidth=0.8)ax.axhline(0, color='gray', linewidth=0.5)for thresh, c in [(-2, '#1a9850'), (-1, '#91cf60'), (1.5, '#fc8d59'), (2.5, '#d73027')]:    ax.axhline(thresh, color=c, linestyle='--', alpha=0.6, linewidth=0.7)ax.set_ylabel('MVRV Z-Score')ax.set_title('MVRV Z-Score (365-day Rolling)')ax.set_ylim(-4.5, 4.5)# Gradientax = axes[2]ax.fill_between(valid.index, 0, valid['mvrv_gradient'],                where=valid['mvrv_gradient'] >= 0, alpha=0.5, color='green', label='Rising')ax.fill_between(valid.index, 0, valid['mvrv_gradient'],                where=valid['mvrv_gradient'] < 0, alpha=0.5, color='red', label='Falling')ax.set_ylabel('Gradient')ax.set_title('MVRV Gradient (30d EMA-smoothed)')ax.legend(fontsize=8)plt.tight_layout()plt.savefig(PLOTS_DIR / 'mvrv_zscore_timeline.png', dpi=150, bbox_inches='tight')plt.show()

# %% [markdown]
# ### 1.5 · Mean Reversion Analysis

# %%
# How quickly does MVRV revert from extreme zones?# For each entry into an extreme zone, measure days until return to neutralextreme_entries = []in_extreme = Falseentry_date = Noneentry_zone = Nonefor date, row in valid.iterrows():    z = row['zone']    if z in ('Deep Value', 'Value', 'Caution', 'Danger') and not in_extreme:        in_extreme = True        entry_date = date        entry_zone = z    elif z == 'Neutral' and in_extreme:        duration = (date - entry_date).days        extreme_entries.append({'entry_date': entry_date, 'exit_date': date,                                'zone': entry_zone, 'duration_days': duration})        in_extreme = Falseexc_df = pd.DataFrame(extreme_entries)if len(exc_df) > 0:    fig, ax = plt.subplots(figsize=(10, 5))    for z in ['Deep Value', 'Value', 'Caution', 'Danger']:        subset = exc_df[exc_df['zone'] == z]        if len(subset) > 0:            ax.hist(subset['duration_days'], bins=30, alpha=0.6,                    color=zone_colors[z], label=f'{z} (n={len(subset)}, med={subset["duration_days"].median():.0f}d)')    ax.set_xlabel('Days Until Reversion to Neutral')    ax.set_ylabel('Count')    ax.set_title('MVRV Zone Duration — How Quickly Does MVRV Revert?')    ax.legend()    plt.tight_layout()    plt.savefig(PLOTS_DIR / 'mvrv_mean_reversion.png', dpi=150, bbox_inches='tight')    plt.show()    print('Reversion statistics:')    print(exc_df.groupby('zone')['duration_days'].describe()[['count','mean','50%','max']])else:    print('No complete zone transitions found')

# %% [markdown]
# ---# 2 · BTC-Specific Polymarket SentimentThe `example_1` model filters to **BTC-related markets only** and computes a sentimentscore from market creation activity. This section explores that signal and goes furtherwith event-window analysis.**Key questions:**- How many Polymarket markets specifically mention Bitcoin?- Does BTC market creation activity correlate with price momentum?- What happens to BTC price around the resolution of major prediction contracts?

# %% [markdown]
# ### 2.1 · Filtering BTC-Related Markets

# %%
markets_df = poly_data.get('markets')if markets_df is not None:    # Filter using same logic as example_1    btc_filter = markets_df.filter(        pl.col('question').str.contains('(?i)bitcoin|btc')    )    print(f'Total markets: {len(markets_df):,}')    print(f'BTC-related markets: {len(btc_filter):,} ({len(btc_filter)/len(markets_df)*100:.1f}%)')        # Category breakdown of BTC markets    if 'category' in btc_filter.columns:        cat_counts = btc_filter.group_by('category').agg(            pl.len().alias('n_markets'),            pl.col('volume').sum().alias('total_vol')        ).sort('total_vol', descending=True)        print(f'\nBTC market categories:')        display(cat_counts)        # Sample questions    print(f'\nSample BTC market questions:')    sample = btc_filter.select('question', 'volume').sort('volume', descending=True).head(10)    display(sample)else:    print('Markets data not loaded')

# %% [markdown]
# ### 2.2 · BTC Sentiment Score Construction

# %%
if markets_df is not None and 'created_at' in btc_filter.columns:    # Convert to pandas for rolling operations    btc_mkts = btc_filter.to_pandas()    btc_mkts['created_date'] = pd.to_datetime(btc_mkts['created_at']).dt.normalize()        daily_stats = btc_mkts.groupby('created_date').agg(        daily_market_count=('market_id', 'count'),        daily_volume=('volume', 'sum')    ).sort_index()        # Rolling 30-day percentiles (same as example_1)    for col in ['daily_market_count', 'daily_volume']:        daily_stats[f'{col}_pct'] = (            daily_stats[col].rolling(30, min_periods=1)            .apply(lambda x: (x.iloc[-1] > x[:-1]).sum() / max(len(x)-1, 1) if len(x) > 1 else 0.5)        )        daily_stats['sentiment'] = (        daily_stats['daily_market_count_pct'] * 0.5 +        daily_stats['daily_volume_pct'] * 0.5    ).fillna(0.5)        print(f'BTC sentiment date range: {daily_stats.index.min().date()} → {daily_stats.index.max().date()}')    print(f'Days with data: {len(daily_stats):,}')    print(f'\nSentiment statistics:')    print(daily_stats['sentiment'].describe())        # Plot sentiment + BTC price    fig, axes = plt.subplots(3, 1, figsize=(14, 9), sharex=True)        # BTC price    price_sub = btc_pd.loc[daily_stats.index.min():daily_stats.index.max()]    axes[0].semilogy(price_sub.index, price_sub['PriceUSD'], color='black', linewidth=0.8)    axes[0].set_ylabel('BTC Price (log)')    axes[0].set_title('BTC Price')        # Market creation activity    axes[1].bar(daily_stats.index, daily_stats['daily_market_count'], width=1, alpha=0.6, color='steelblue')    axes[1].set_ylabel('New BTC Markets/Day')    axes[1].set_title('Daily BTC Market Creation on Polymarket')        # Sentiment score    axes[2].plot(daily_stats.index, daily_stats['sentiment'], color='purple', linewidth=0.8)    axes[2].axhline(0.5, color='gray', linestyle='--', linewidth=0.5)    axes[2].fill_between(daily_stats.index, 0.5, daily_stats['sentiment'],                         where=daily_stats['sentiment'] >= 0.5, alpha=0.3, color='green')    axes[2].fill_between(daily_stats.index, 0.5, daily_stats['sentiment'],                         where=daily_stats['sentiment'] < 0.5, alpha=0.3, color='red')    axes[2].set_ylabel('Sentiment Score')    axes[2].set_title('BTC Polymarket Sentiment (30-day rolling percentile)')    axes[2].set_ylim(0, 1)        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'polymarket_btc_sentiment.png', dpi=150, bbox_inches='tight')    plt.show()

# %% [markdown]
# ### 2.3 · Sentiment vs BTC Returns (Lead/Lag)

# %%
# Merge sentiment with BTC daily returnsif 'daily_stats' in dir() and len(daily_stats) > 0:    merged = btc_pd[['daily_ret', 'PriceUSD']].join(daily_stats[['sentiment']], how='inner').dropna()    print(f'Merged rows: {len(merged):,}')        # Lead/lag correlation    lags = range(-14, 15)    corrs = [merged['sentiment'].shift(lag).corr(merged['daily_ret']) for lag in lags]        fig, ax = plt.subplots(figsize=(10, 5))    colors = ['green' if l > 0 else 'red' if l < 0 else 'black' for l in lags]    ax.bar(lags, corrs, color='steelblue', alpha=0.7, edgecolor='white')    ax.axhline(0, color='black', linewidth=0.5)    ax.axvline(0, color='red', linewidth=0.8, linestyle='--', alpha=0.5)    ax.set_xlabel('Lag (positive = sentiment leads returns)')    ax.set_ylabel('Pearson Correlation')    ax.set_title('BTC-Specific Polymarket Sentiment vs BTC Daily Returns')        # Annotate strongest    max_idx = np.argmax(np.abs(corrs))    ax.annotate(f'Best: lag={list(lags)[max_idx]}, r={corrs[max_idx]:.4f}',                xy=(list(lags)[max_idx], corrs[max_idx]),                xytext=(5, 15), textcoords='offset points',                arrowprops=dict(arrowstyle='->', color='red'),                fontsize=9, color='red')        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'sentiment_leadlag.png', dpi=150, bbox_inches='tight')    plt.show()        # Spearman for robustness    for lag in [0, 1, 3, 7]:        r, p = stats.spearmanr(merged['sentiment'].shift(lag).dropna(),                               merged['daily_ret'].iloc[lag:] if lag > 0 else merged['daily_ret'].iloc[:len(merged)-abs(lag) if lag < 0 else len(merged)])        print(f'  Spearman lag={lag:+2d}: r={r:.4f}, p={p:.4f}')

# %% [markdown]
# ### 2.4 · Category-Level Signal Decomposition

# %%
# Do different PM categories have different predictive power for BTC?if markets_df is not None and 'category' in markets_df.columns:    all_mkts = markets_df.to_pandas()    all_mkts['created_date'] = pd.to_datetime(all_mkts['created_at']).dt.normalize()        top_cats = all_mkts['category'].value_counts().head(6).index.tolist()        fig, axes = plt.subplots(2, 3, figsize=(16, 8))    axes = axes.flatten()        for ax, cat in zip(axes, top_cats):        cat_daily = all_mkts[all_mkts['category'] == cat].groupby('created_date').agg(            count=('market_id', 'count')        ).sort_index()                cat_daily['count_pct'] = cat_daily['count'].rolling(30, min_periods=1).apply(            lambda x: (x.iloc[-1] > x[:-1]).sum() / max(len(x)-1, 1) if len(x) > 1 else 0.5        )                merged_cat = btc_pd[['daily_ret']].join(cat_daily[['count_pct']], how='inner').dropna()                if len(merged_cat) > 20:            lags_short = range(0, 8)            c = [merged_cat['count_pct'].shift(l).corr(merged_cat['daily_ret']) for l in lags_short]            ax.bar(lags_short, c, color='steelblue', alpha=0.7)            ax.axhline(0, color='black', linewidth=0.5)            ax.set_title(f'{cat}', fontsize=10)            ax.set_xlabel('Lag (days)')            ax.set_ylabel('Corr')        else:            ax.text(0.5, 0.5, 'Insufficient overlap', ha='center', va='center', transform=ax.transAxes)            ax.set_title(f'{cat}', fontsize=10)        plt.suptitle('Lead/Lag Correlations by Polymarket Category → BTC Returns', fontsize=13, y=1.02)    plt.tight_layout()    plt.savefig(PLOTS_DIR / 'category_leadlag.png', dpi=150, bbox_inches='tight')    plt.show()

# %% [markdown]
# ---# 3 · Volatility & Regime AnalysisThe outline explicitly suggests a `volatility_deep_dive.ipynb`. This section formalisesvolatility regime detection and explores how on-chain features behave differently acrossmarket regimes.**Key questions:**- Can we define discrete volatility regimes (low/medium/high)?- Do on-chain metrics behave differently in each regime?- Does net exchange flow correlate with regime transitions?

# %% [markdown]
# ### 3.1 · Volatility Regime Classification

# %%
# Use 30-day realised vol to define regimes via quantilesvol_col = Nonefor c in ['VtyDayRet30d', 'vol_30d']:    if c in btc_pd.columns:        vol_col = c; breakif vol_col is None:    # Compute from returns    btc_pd['vol_30d'] = btc_pd['daily_ret'].rolling(30).std() * np.sqrt(365) * 100    vol_col = 'vol_30d'vol_data = btc_pd.dropna(subset=[vol_col]).copy()q33, q67 = vol_data[vol_col].quantile([0.33, 0.67])def classify_vol(v):    if v <= q33: return 'Low Vol'    elif v <= q67: return 'Med Vol'    else: return 'High Vol'vol_data['vol_regime'] = vol_data[vol_col].apply(classify_vol)regime_colors = {'Low Vol': '#2166ac', 'Med Vol': '#f4a582', 'High Vol': '#b2182b'}fig, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)# Price with regime overlayax = axes[0]ax.semilogy(vol_data.index, vol_data['PriceUSD'], color='black', linewidth=0.8)for regime in ['Low Vol', 'Med Vol', 'High Vol']:    mask = vol_data['vol_regime'] == regime    ax.fill_between(vol_data.index, vol_data['PriceUSD'].min(), vol_data['PriceUSD'].max(),                    where=mask, alpha=0.15, color=regime_colors[regime], label=regime)ax.set_ylabel('BTC Price (log)')ax.set_title('BTC Price with Volatility Regime Overlay')ax.legend(fontsize=9)# Volatility time seriesax = axes[1]ax.plot(vol_data.index, vol_data[vol_col], color='gray', linewidth=0.5, alpha=0.5)ax.axhline(q33, color='blue', linestyle='--', alpha=0.5, label=f'33rd pctile ({q33:.4f})')ax.axhline(q67, color='red', linestyle='--', alpha=0.5, label=f'67th pctile ({q67:.4f})')ax.set_ylabel('30-Day Realised Volatility')ax.set_title('Volatility with Regime Thresholds')ax.legend(fontsize=8)plt.tight_layout()plt.savefig(PLOTS_DIR / 'volatility_regimes.png', dpi=150, bbox_inches='tight')plt.show()print(f'Regime distribution:')print(vol_data['vol_regime'].value_counts())

# %% [markdown]
# ### 3.2 · On-Chain Feature Behaviour by Regime

# %%
# How do key on-chain metrics differ across volatility regimes?feature_cols = ['CapMVRVCur', 'HashRate', 'FlowInExUSD', 'FlowOutExUSD',                'TxCnt', 'FeeTotUSD']avail_feats = [c for c in feature_cols if c in vol_data.columns]if avail_feats:    n = len(avail_feats)    fig, axes = plt.subplots(2, 3, figsize=(16, 9))    axes = axes.flatten()        for i, feat in enumerate(avail_feats):        ax = axes[i]        for regime in ['Low Vol', 'Med Vol', 'High Vol']:            subset = vol_data[vol_data['vol_regime'] == regime][feat].dropna()            if len(subset) > 10:                ax.hist(subset, bins=40, alpha=0.4, color=regime_colors[regime],                        label=regime, density=True)        ax.set_title(feat, fontsize=10)        ax.set_ylabel('Density')        ax.legend(fontsize=7)        # Fill empty axes    for j in range(len(avail_feats), len(axes)):        axes[j].axis('off')        plt.suptitle('On-Chain Feature Distributions by Volatility Regime', fontsize=13, y=1.01)    plt.tight_layout()    plt.savefig(PLOTS_DIR / 'features_by_regime.png', dpi=150, bbox_inches='tight')    plt.show()        # Statistical test: Kruskal-Wallis for each feature    print(f'Kruskal-Wallis tests (feature distributions differ across regimes?):')    print(f'{"Feature":>16s}  {"H-stat":>8s}  {"p-value":>10s}  {"Sig?":>5s}')    for feat in avail_feats:        groups = [vol_data[vol_data['vol_regime'] == r][feat].dropna().values                  for r in ['Low Vol', 'Med Vol', 'High Vol']]        groups = [g for g in groups if len(g) > 5]        if len(groups) >= 2:            stat, pval = stats.kruskal(*groups)            sig = '✅' if pval < 0.05 else '❌'            print(f'{feat:>16s}  {stat:>8.1f}  {pval:>10.2e}  {sig:>5s}')

# %% [markdown]
# ### 3.3 · Net Exchange Flow Analysis

# %%
# Net exchange flow = InFlow - OutFlow (positive = BTC moving TO exchanges = selling pressure)if 'FlowInExUSD' in btc_pd.columns and 'FlowOutExUSD' in btc_pd.columns:    btc_pd['NetFlowExUSD'] = btc_pd['FlowInExUSD'] - btc_pd['FlowOutExUSD']    btc_pd['NetFlowExUSD_30d'] = btc_pd['NetFlowExUSD'].rolling(30).mean()        flow_data = btc_pd.dropna(subset=['NetFlowExUSD', 'PriceUSD']).copy()        fig, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)        axes[0].semilogy(flow_data.index, flow_data['PriceUSD'], color='black', linewidth=0.8)    axes[0].set_ylabel('BTC Price (log)')    axes[0].set_title('BTC Price')        axes[1].fill_between(flow_data.index, 0, flow_data['NetFlowExUSD_30d'],                         where=flow_data['NetFlowExUSD_30d'] >= 0,                         alpha=0.5, color='red', label='Net Inflow (selling pressure)')    axes[1].fill_between(flow_data.index, 0, flow_data['NetFlowExUSD_30d'],                         where=flow_data['NetFlowExUSD_30d'] < 0,                         alpha=0.5, color='green', label='Net Outflow (accumulation)')    axes[1].set_ylabel('Net Exchange Flow (USD, 30d MA)')    axes[1].set_title('Net Exchange Flow — Accumulation vs Distribution')    axes[1].legend()    axes[1].axhline(0, color='black', linewidth=0.5)        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'net_exchange_flow.png', dpi=150, bbox_inches='tight')    plt.show()        # Correlation: net flow vs forward returns    print('Net exchange flow (30d MA) vs forward BTC returns:')    for h in [7, 30, 90]:        col = f'fwd_ret_{h}d'        if col in flow_data.columns:            valid_pair = flow_data[['NetFlowExUSD_30d', col]].dropna()            r, p = stats.spearmanr(valid_pair['NetFlowExUSD_30d'], valid_pair[col])            print(f'  {h:3d}d forward: Spearman r={r:.4f}, p={p:.4f}')

# %% [markdown]
# ---# 4 · Additional Signal ExplorationBeyond MVRV and Polymarket, the CoinMetrics data contains several under-exploredmetrics that may provide complementary accumulation signals.

# %% [markdown]
# ### 4.1 · Fee Market as Momentum Indicator

# %%
# High fees ≈ network congestion ≈ high demand ≈ potential momentum signalif 'FeeTotUSD' in btc_pd.columns:    btc_pd['fee_zscore'] = (        (btc_pd['FeeTotUSD'] - btc_pd['FeeTotUSD'].rolling(365, min_periods=90).mean())        / btc_pd['FeeTotUSD'].rolling(365, min_periods=90).std()    ).clip(-3, 3)        fee_data = btc_pd.dropna(subset=['fee_zscore', 'fwd_ret_30d'])        # Scatter: fee z-score vs forward returns    fig, axes = plt.subplots(1, 2, figsize=(14, 5))        axes[0].scatter(fee_data['fee_zscore'], fee_data['fwd_ret_30d'] * 100,                    alpha=0.1, s=5, color='steelblue')    axes[0].set_xlabel('Fee Z-Score (365d)')    axes[0].set_ylabel('30-Day Forward Return (%)')    axes[0].set_title('Fee Market Activity vs Forward BTC Returns')    axes[0].axhline(0, color='gray', linewidth=0.5)    axes[0].axvline(0, color='gray', linewidth=0.5)        r, p = stats.spearmanr(fee_data['fee_zscore'], fee_data['fwd_ret_30d'])    axes[0].annotate(f'Spearman r={r:.3f}, p={p:.3f}', xy=(0.05, 0.95),                     xycoords='axes fraction', fontsize=9)        # Fee z-score over time    valid_fee = btc_pd.dropna(subset=['fee_zscore'])    axes[1].plot(valid_fee.index, valid_fee['fee_zscore'], linewidth=0.6, color='coral')    axes[1].axhline(0, color='gray', linewidth=0.5)    axes[1].set_xlabel('Date')    axes[1].set_ylabel('Fee Z-Score')    axes[1].set_title('Fee Market Z-Score Over Time')        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'fee_market_signal.png', dpi=150, bbox_inches='tight')    plt.show()

# %% [markdown]
# ### 4.2 · Exchange Supply Ratio

# %%
# Exchange supply as % of total — declining ratio = accumulation eraif 'SplyExNtv' in btc_pd.columns and 'SplyCur' in btc_pd.columns:    btc_pd['exchange_supply_pct'] = (btc_pd['SplyExNtv'] / btc_pd['SplyCur'] * 100)        ex_data = btc_pd.dropna(subset=['exchange_supply_pct', 'PriceUSD'])        fig, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)        axes[0].semilogy(ex_data.index, ex_data['PriceUSD'], color='black', linewidth=0.8)    axes[0].set_ylabel('BTC Price (log)')    axes[0].set_title('BTC Price')        axes[1].plot(ex_data.index, ex_data['exchange_supply_pct'], color='#e7298a', linewidth=0.8)    axes[1].set_ylabel('Exchange Supply (%)')    axes[1].set_title('BTC Held on Exchanges (% of Circulating Supply)')    axes[1].fill_between(ex_data.index, ex_data['exchange_supply_pct'].min(),                         ex_data['exchange_supply_pct'], alpha=0.2, color='#e7298a')        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'exchange_supply_ratio.png', dpi=150, bbox_inches='tight')    plt.show()        # Correlation with forward returns    for h in [30, 90]:        col = f'fwd_ret_{h}d'        if col in ex_data.columns:            pair = ex_data[['exchange_supply_pct', col]].dropna()            r, p = stats.spearmanr(pair['exchange_supply_pct'], pair[col])            print(f'  Exchange supply % vs {h}d fwd return: r={r:.4f}, p={p:.4f}')else:    print('Exchange supply columns not available')

# %% [markdown]
# ### 4.3 · Bitcoin Halving Cycle Analysis

# %%
# Known Bitcoin halving dateshalvings = pd.to_datetime(['2012-11-28', '2016-07-09', '2020-05-11', '2024-04-19'])btc_pd['days_since_halving'] = np.nanfor i, h in enumerate(halvings):    next_h = halvings[i+1] if i+1 < len(halvings) else pd.Timestamp('2028-04-01')    mask = (btc_pd.index >= h) & (btc_pd.index < next_h)    btc_pd.loc[mask, 'days_since_halving'] = (btc_pd.index[mask] - h).daysbtc_pd['cycle_phase'] = pd.cut(btc_pd['days_since_halving'],                                bins=[0, 180, 365, 730, 1460],                                labels=['0-6mo', '6-12mo', '12-24mo', '24-48mo'],                                include_lowest=True)cycle_data = btc_pd.dropna(subset=['cycle_phase', 'fwd_ret_30d'])if len(cycle_data) > 50:    fig, ax = plt.subplots(figsize=(10, 5))    phase_order = ['0-6mo', '6-12mo', '12-24mo', '24-48mo']    phase_colors = ['#d73027', '#fc8d59', '#91cf60', '#1a9850']        box_data = [cycle_data[cycle_data['cycle_phase'] == p]['fwd_ret_30d'].values * 100                for p in phase_order]    bp = ax.boxplot(box_data, labels=phase_order, patch_artist=True, showmeans=True,                    meanprops=dict(marker='D', markerfacecolor='black', markersize=5))    for patch, c in zip(bp['boxes'], phase_colors):        patch.set_facecolor(c)        patch.set_alpha(0.6)        ax.set_xlabel('Phase After Halving')    ax.set_ylabel('30-Day Forward Return (%)')    ax.set_title('BTC Forward Returns by Halving Cycle Phase')    ax.axhline(0, color='gray', linewidth=0.5)        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'halving_cycle.png', dpi=150, bbox_inches='tight')    plt.show()        # Stats    print('Mean 30d forward return by halving phase:')    for p in phase_order:        sub = cycle_data[cycle_data['cycle_phase'] == p]['fwd_ret_30d']        print(f'  {p}: mean={sub.mean()*100:+.2f}%, n={len(sub)}')

# %% [markdown]
# ---# 5 · Polymarket Odds DynamicsThe `odds_history` table (2.1M rows) contains time-series probability snapshots forevery market. This section analyses **probability velocity** — how fast odds change —as a real-time sentiment signal, focusing on BTC-related markets.

# %% [markdown]
# ### 5.1 · Odds Velocity in BTC Markets

# %%
# Compute daily probability changes for BTC-related marketsodds_df = poly_data.get('odds')if odds_df is not None and markets_df is not None:    # Get BTC market IDs    btc_ids = set(btc_filter.select('market_id').to_series().to_list())    print(f'BTC market IDs: {len(btc_ids):,}')        # Filter odds to BTC markets    btc_odds = odds_df.filter(pl.col('market_id').is_in(list(btc_ids)))    print(f'BTC odds rows: {len(btc_odds):,} out of {len(odds_df):,} total')        if len(btc_odds) > 100:        # Daily aggregation: mean price, std (dispersion), count        btc_odds_daily = (btc_odds            .with_columns(pl.col('timestamp').dt.date().alias('date'))            .group_by('date')            .agg([                pl.col('price').mean().alias('mean_prob'),                pl.col('price').std().alias('prob_std'),                pl.len().alias('n_snapshots'),            ])            .sort('date')            .to_pandas())        btc_odds_daily['date'] = pd.to_datetime(btc_odds_daily['date'])        btc_odds_daily = btc_odds_daily.set_index('date')                # Probability velocity = day-over-day change in mean probability        btc_odds_daily['prob_velocity'] = btc_odds_daily['mean_prob'].diff().abs()        btc_odds_daily['prob_velocity_7d'] = btc_odds_daily['prob_velocity'].rolling(7).mean()                fig, axes = plt.subplots(3, 1, figsize=(14, 9), sharex=True)                # BTC price        price_sub = btc_pd.loc[btc_odds_daily.index.min():btc_odds_daily.index.max()]        axes[0].semilogy(price_sub.index, price_sub['PriceUSD'], color='black', linewidth=0.8)        axes[0].set_ylabel('BTC Price (log)')        axes[0].set_title('BTC Price')                # Probability dispersion (cross-market disagreement)        axes[1].plot(btc_odds_daily.index, btc_odds_daily['prob_std'],                     color='purple', linewidth=0.7, alpha=0.7)        axes[1].set_ylabel('Probability Std Dev')        axes[1].set_title('BTC Market Probability Dispersion (Cross-Market Disagreement)')                # Velocity        axes[2].fill_between(btc_odds_daily.index, 0, btc_odds_daily['prob_velocity_7d'],                             alpha=0.5, color='orange')        axes[2].set_ylabel('|ΔProbability| (7d MA)')        axes[2].set_title('Odds Velocity — How Fast Are BTC Market Probabilities Changing?')                plt.tight_layout()        plt.savefig(PLOTS_DIR / 'odds_velocity.png', dpi=150, bbox_inches='tight')        plt.show()                # Correlation with BTC returns        merged_odds = btc_pd[['daily_ret']].join(btc_odds_daily[['prob_velocity_7d', 'prob_std']], how='inner').dropna()        for feat in ['prob_velocity_7d', 'prob_std']:            for lag in [0, 1, 3, 7]:                pair = merged_odds[[feat, 'daily_ret']].copy()                pair[feat] = pair[feat].shift(lag)                pair = pair.dropna()                if len(pair) > 20:                    r, p = stats.spearmanr(pair[feat], pair['daily_ret'])                    print(f'  {feat} lag={lag}: r={r:.4f}, p={p:.4f}')else:    print('Odds data not available')

# %% [markdown]
# ---# 6 · Network Growth & Address Activity`AdrActCnt` (active addresses) and `AdrBalCnt` (addresses with balance) are completelyunexplored in the base EDA. These are fundamental network health metrics.

# %% [markdown]
# ### 6.1 · Active Address Trends

# %%
if 'AdrActCnt' in btc_pd.columns and 'AdrBalCnt' in btc_pd.columns:    addr_data = btc_pd.dropna(subset=['AdrActCnt', 'AdrBalCnt', 'PriceUSD']).copy()        # Engagement ratio: active / total balance addresses    addr_data['engagement_ratio'] = addr_data['AdrActCnt'] / addr_data['AdrBalCnt'].replace(0, np.nan)    addr_data['adr_act_30d'] = addr_data['AdrActCnt'].rolling(30).mean()    addr_data['adr_bal_growth'] = addr_data['AdrBalCnt'].pct_change(30)  # 30-day growth rate        fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)        axes[0].semilogy(addr_data.index, addr_data['PriceUSD'], color='black', linewidth=0.8, label='Price')    ax2 = axes[0].twinx()    ax2.plot(addr_data.index, addr_data['adr_act_30d'], color='steelblue', linewidth=0.7, alpha=0.7, label='Active (30d MA)')    ax2.set_ylabel('Active Addresses', color='steelblue')    axes[0].set_ylabel('BTC Price (log)')    axes[0].set_title('BTC Price vs Active Addresses')        axes[1].semilogy(addr_data.index, addr_data['AdrBalCnt'], color='#2ca02c', linewidth=0.7)    axes[1].set_ylabel('Addresses with Balance')    axes[1].set_title('Total Non-Zero Balance Addresses (Network Adoption)')        axes[2].plot(addr_data.index, addr_data['engagement_ratio'], color='coral', linewidth=0.6, alpha=0.7)    axes[2].set_ylabel('Active / Total Ratio')    axes[2].set_title('Network Engagement Ratio (Active Addresses / Total Balance Addresses)')    axes[2].rolling_mean = addr_data['engagement_ratio'].rolling(30).mean()    axes[2].plot(addr_data.index, addr_data['engagement_ratio'].rolling(30).mean(),                 color='red', linewidth=1.2, label='30d MA')    axes[2].legend()        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'address_activity.png', dpi=150, bbox_inches='tight')    plt.show()        # Correlation with forward returns    for feat, label in [('engagement_ratio', 'Engagement Ratio'),                         ('adr_bal_growth', 'Address Growth (30d)')]:        if feat in addr_data.columns:            pair = addr_data[[feat, 'fwd_ret_30d']].dropna()            if len(pair) > 50:                r, p = stats.spearmanr(pair[feat], pair['fwd_ret_30d'])                print(f'  {label} vs 30d fwd return: r={r:.4f}, p={p:.4f}')else:    print('Address columns not available')

# %% [markdown]
# ---# 7 · Derived Metrics — NUPL, Volume Turnover & Hash RibbonThese metrics are computable from existing columns but not directly available.

# %% [markdown]
# ### 7.1 · NUPL (Net Unrealized Profit/Loss)

# %%
# NUPL = 1 - 1/MVRV = (Market Cap - Realized Cap) / Market Cap# NUPL < 0 = "capitulation" — historically the best accumulation zoneif 'CapMVRVCur' in btc_pd.columns:    btc_pd['nupl'] = 1 - 1 / btc_pd['CapMVRVCur'].replace(0, np.nan)        nupl_data = btc_pd.dropna(subset=['nupl', 'PriceUSD'])        fig, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)        axes[0].semilogy(nupl_data.index, nupl_data['PriceUSD'], color='black', linewidth=0.8)    axes[0].set_ylabel('BTC Price (log)')    axes[0].set_title('BTC Price')        # NUPL with zone shading    ax = axes[1]    ax.plot(nupl_data.index, nupl_data['nupl'], color='steelblue', linewidth=0.7)    ax.axhline(0, color='black', linewidth=0.8)    ax.fill_between(nupl_data.index, nupl_data['nupl'].min(), 0,                    where=nupl_data['nupl'] < 0, alpha=0.3, color='green', label='Capitulation (NUPL<0)')    ax.fill_between(nupl_data.index, 0, 0.5,                    where=(nupl_data['nupl'] >= 0) & (nupl_data['nupl'] < 0.5),                    alpha=0.15, color='yellow', label='Hope/Fear (0-0.5)')    ax.fill_between(nupl_data.index, 0.5, nupl_data['nupl'].max(),                    where=nupl_data['nupl'] >= 0.5, alpha=0.15, color='red', label='Euphoria (>0.5)')    ax.set_ylabel('NUPL')    ax.set_title('Net Unrealized Profit/Loss (derived from MVRV)')    ax.legend(fontsize=8)        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'nupl.png', dpi=150, bbox_inches='tight')    plt.show()        # Forward return by NUPL zone    nupl_valid = btc_pd.dropna(subset=['nupl', 'fwd_ret_30d'])    for zone, lo, hi in [('Capitulation', -10, 0), ('Hope', 0, 0.25),                          ('Belief', 0.25, 0.5), ('Euphoria', 0.5, 0.75), ('Greed', 0.75, 10)]:        mask = (nupl_valid['nupl'] >= lo) & (nupl_valid['nupl'] < hi)        sub = nupl_valid.loc[mask, 'fwd_ret_30d']        if len(sub) > 10:            print(f'  {zone:>12s}: mean 30d fwd = {sub.mean()*100:+.2f}%, n={len(sub)}')

# %% [markdown]
# ### 7.2 · Volume Turnover Ratio

# %%
# Turnover = daily volume / market cap — measures liquidity/interestif 'volume_reported_spot_usd_1d' in btc_pd.columns and 'CapMrktCurUSD' in btc_pd.columns:    btc_pd['turnover'] = btc_pd['volume_reported_spot_usd_1d'] / btc_pd['CapMrktCurUSD'].replace(0, np.nan)    btc_pd['turnover_30d'] = btc_pd['turnover'].rolling(30).mean()    btc_pd['turnover_zscore'] = (        (btc_pd['turnover_30d'] - btc_pd['turnover_30d'].rolling(365, min_periods=90).mean())        / btc_pd['turnover_30d'].rolling(365, min_periods=90).std()    ).clip(-3, 3)        turn_data = btc_pd.dropna(subset=['turnover_30d', 'PriceUSD'])        fig, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)        axes[0].semilogy(turn_data.index, turn_data['PriceUSD'], color='black', linewidth=0.8)    axes[0].set_ylabel('BTC Price (log)')    axes[0].set_title('BTC Price')        axes[1].plot(turn_data.index, turn_data['turnover_30d'] * 100, color='teal', linewidth=0.7)    axes[1].set_ylabel('Daily Turnover % (30d MA)')    axes[1].set_title('Volume Turnover Ratio — Liquidity & Market Interest')        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'volume_turnover.png', dpi=150, bbox_inches='tight')    plt.show()        # Correlation    pair = btc_pd[['turnover_zscore', 'fwd_ret_30d']].dropna()    if len(pair) > 50:        r, p = stats.spearmanr(pair['turnover_zscore'], pair['fwd_ret_30d'])        print(f'Turnover Z-Score vs 30d fwd return: r={r:.4f}, p={p:.4f}')else:    print('Volume or market cap columns not available')

# %% [markdown]
# ### 7.3 · Hash Ribbon — Miner Capitulation Signal

# %%
# Hash Ribbon: 30d MA vs 60d MA of hash rate# When 30d < 60d → miner capitulation (miners shutting off = selling pressure ending)# When 30d crosses back above 60d → recovery signal = buyif 'HashRate' in btc_pd.columns:    btc_pd['hash_30d'] = btc_pd['HashRate'].rolling(30).mean()    btc_pd['hash_60d'] = btc_pd['HashRate'].rolling(60).mean()    btc_pd['hash_ribbon'] = btc_pd['hash_30d'] / btc_pd['hash_60d'].replace(0, np.nan) - 1    # Positive = recovery, Negative = capitulation        # Drawdown from all-time high    btc_pd['hash_ath'] = btc_pd['HashRate'].expanding().max()    btc_pd['hash_drawdown'] = btc_pd['HashRate'] / btc_pd['hash_ath'] - 1        hash_data = btc_pd.dropna(subset=['hash_ribbon', 'PriceUSD'])        fig, axes = plt.subplots(3, 1, figsize=(14, 9), sharex=True)        axes[0].semilogy(hash_data.index, hash_data['PriceUSD'], color='black', linewidth=0.8)    axes[0].set_ylabel('BTC Price (log)')    axes[0].set_title('BTC Price')        # Hash rate with MAs    axes[1].semilogy(hash_data.index, hash_data['HashRate'], color='gray', linewidth=0.4, alpha=0.4, label='Raw')    axes[1].semilogy(hash_data.index, hash_data['hash_30d'], color='blue', linewidth=0.8, label='30d MA')    axes[1].semilogy(hash_data.index, hash_data['hash_60d'], color='red', linewidth=0.8, label='60d MA')    axes[1].set_ylabel('Hash Rate (TH/s)')    axes[1].set_title('Hash Rate with 30d/60d Moving Averages')    axes[1].legend(fontsize=8)        # Hash ribbon signal    axes[2].fill_between(hash_data.index, 0, hash_data['hash_ribbon'],                         where=hash_data['hash_ribbon'] >= 0, alpha=0.5, color='green', label='Recovery')    axes[2].fill_between(hash_data.index, 0, hash_data['hash_ribbon'],                         where=hash_data['hash_ribbon'] < 0, alpha=0.5, color='red', label='Capitulation')    axes[2].axhline(0, color='black', linewidth=0.5)    axes[2].set_ylabel('Hash Ribbon (30d/60d - 1)')    axes[2].set_title('Hash Ribbon — Miner Capitulation & Recovery')    axes[2].legend(fontsize=8)        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'hash_ribbon.png', dpi=150, bbox_inches='tight')    plt.show()        # Forward returns during capitulation vs recovery    hr_valid = btc_pd.dropna(subset=['hash_ribbon', 'fwd_ret_30d'])    for label, cond in [('Capitulation (HR<0)', hr_valid['hash_ribbon'] < 0),                         ('Recovery (HR>0)', hr_valid['hash_ribbon'] >= 0)]:        sub = hr_valid.loc[cond, 'fwd_ret_30d']        if len(sub) > 10:            print(f'  {label}: mean 30d fwd = {sub.mean()*100:+.2f}%, n={len(sub)}')        r, p = stats.spearmanr(hr_valid['hash_ribbon'], hr_valid['fwd_ret_30d'])    print(f'  Hash Ribbon vs 30d fwd return: r={r:.4f}, p={p:.4f}')

# %% [markdown]
# ---# 8 · Time-Series DiagnosticsAutocorrelation structure justifies lag choices. Rolling correlation stabilityassesses whether feature relationships are robust over time.

# %% [markdown]
# ### 8.1 · Autocorrelation & Partial Autocorrelation of Returns

# %%
from statsmodels.graphics.tsaplots import plot_acf, plot_pacfrets = btc_pd['daily_ret'].dropna()fig, axes = plt.subplots(2, 2, figsize=(14, 8))# Daily returns ACF/PACFplot_acf(rets, lags=40, ax=axes[0, 0], title='ACF — Daily Returns')plot_pacf(rets, lags=40, ax=axes[0, 1], title='PACF — Daily Returns', method='ywm')# Absolute returns (volatility clustering)plot_acf(rets.abs(), lags=40, ax=axes[1, 0], title='ACF — |Daily Returns| (Volatility Clustering)')plot_pacf(rets.abs(), lags=40, ax=axes[1, 1], title='PACF — |Daily Returns|', method='ywm')plt.tight_layout()plt.savefig(PLOTS_DIR / 'autocorrelation.png', dpi=150, bbox_inches='tight')plt.show()print('Key insight: If |returns| show strong ACF, volatility is predictable (clustering).')print('This justifies using volatility regime features in the model.')

# %% [markdown]
# ### 8.2 · Rolling Correlation Stability

# %%
# Are feature→return correlations stable over time?# If correlations shift drastically, the model needs regime-awarenessstability_feats = {    'mvrv_zscore': 'MVRV Z-Score',    'NetFlowExUSD_30d': 'Net Exchange Flow',    'fee_zscore': 'Fee Z-Score',    'exchange_supply_pct': 'Exchange Supply %',}avail_stab = {k: v for k, v in stability_feats.items() if k in btc_pd.columns}target = 'fwd_ret_30d'window = 365  # 1-year rolling windowif avail_stab:    fig, ax = plt.subplots(figsize=(14, 6))        for feat_col, label in avail_stab.items():        pair = btc_pd[[feat_col, target]].dropna()        if len(pair) > window:            rolling_corr = pair[feat_col].rolling(window).corr(pair[target])            ax.plot(pair.index, rolling_corr, linewidth=0.8, alpha=0.8, label=label)        ax.axhline(0, color='black', linewidth=0.5)    ax.set_ylabel('Rolling Spearman Correlation')    ax.set_title(f'Rolling {window}-Day Correlation with 30-Day Forward Returns')    ax.set_ylim(-0.5, 0.5)    ax.legend(fontsize=9)        plt.tight_layout()    plt.savefig(PLOTS_DIR / 'rolling_correlation_stability.png', dpi=150, bbox_inches='tight')    plt.show()        # Stability metric: std of rolling correlation    print(f'Correlation stability (lower std = more stable):')    for feat_col, label in avail_stab.items():        pair = btc_pd[[feat_col, target]].dropna()        if len(pair) > window:            rc = pair[feat_col].rolling(window).corr(pair[target]).dropna()            print(f'  {label:>25s}: mean_r={rc.mean():+.3f}, std={rc.std():.3f}, range=[{rc.min():+.3f}, {rc.max():+.3f}]')

# %% [markdown]
# ---# 9 · Feature Importance Ranking — SynthesisThis section ranks all explored features by their predictive power for BTC forwardreturns, providing a data-driven justification for signal weights in the model.

# %%
# Rank all features by absolute Spearman correlation with 30d forward returnscandidates = {    'mvrv_zscore': 'MVRV Z-Score',    'mvrv_gradient': 'MVRV Gradient',    'mvrv_acceleration': 'MVRV Acceleration',    'nupl': 'NUPL',    'NetFlowExUSD_30d': 'Net Exchange Flow (30d)',    'fee_zscore': 'Fee Market Z-Score',    'exchange_supply_pct': 'Exchange Supply %',    'days_since_halving': 'Days Since Halving',    'hash_ribbon': 'Hash Ribbon',    'turnover_zscore': 'Volume Turnover Z-Score',    'engagement_ratio': 'Address Engagement Ratio',    'adr_bal_growth': 'Address Balance Growth (30d)',}# Also add volatilityif vol_col in btc_pd.columns:    candidates[vol_col] = 'Realised Volatility (30d)'target = 'fwd_ret_30d'results = []for feat_col, label in candidates.items():    if feat_col in btc_pd.columns:        pair = btc_pd[[feat_col, target]].dropna()        if len(pair) > 50:            r, p = stats.spearmanr(pair[feat_col], pair[target])            results.append({'Feature': label, 'Column': feat_col,                           'Spearman_r': r, 'abs_r': abs(r), 'p_value': p, 'n': len(pair)})results_df = pd.DataFrame(results).sort_values('abs_r', ascending=False)display(results_df[['Feature', 'Spearman_r', 'p_value', 'n']].reset_index(drop=True))# Plotfig, ax = plt.subplots(figsize=(10, max(6, len(results_df) * 0.45)))colors = ['#1a9850' if r < 0 else '#d73027' for r in results_df['Spearman_r']]ax.barh(results_df['Feature'], results_df['Spearman_r'], color=colors, alpha=0.7, edgecolor='white')ax.axvline(0, color='black', linewidth=0.5)ax.set_xlabel('Spearman r (vs 30-day forward BTC return)')ax.set_title('Feature Ranking — Predictive Power for BTC Accumulation Timing')ax.invert_yaxis()# Significance markersfor i, (_, row) in enumerate(results_df.iterrows()):    marker = '***' if row['p_value'] < 0.001 else '**' if row['p_value'] < 0.01 else '*' if row['p_value'] < 0.05 else ''    ax.text(row['Spearman_r'] + 0.002 * np.sign(row['Spearman_r']),            i, marker, va='center', fontsize=10, fontweight='bold')plt.tight_layout()plt.savefig(PLOTS_DIR / 'feature_importance_ranking.png', dpi=150, bbox_inches='tight')plt.show()

# %% [markdown]
# ---# 10 · Key Takeaways & Model Implications## Summary of Findings| Signal | Predictive Power | Model Implication ||--------|-----------------|-------------------|| **MVRV Z-Score** | Forward returns differ significantly by zone | Justifies dominant signal weight (64%) || **NUPL** | Classic capitulation indicator derived from MVRV | Strongly correlated — use zones for regime context || **MVRV Gradient** | Trend direction adds incremental value | Use as directional modifier || **Net Exchange Flow** | Negative flow (accumulation) correlates with future gains | Strong new feature candidate || **Hash Ribbon** | Miner capitulation → recovery = buy signal | Structural buy signal || **Exchange Supply %** | Declining exchange reserves = bullish long-term signal | Slow-moving regime context || **Volume Turnover** | Low turnover + low MVRV = apathy bottom | Complementary liquidity signal || **Fee Market Z-Score** | Network activity spikes precede momentum | Momentum confirmation signal || **Address Engagement** | Network growth rate as adoption proxy | Long-term structural feature || **Halving Cycle Phase** | Returns vary by cycle position | Structural feature for model || **Polymarket Sentiment** | BTC-specific sentiment shows weak but present signal | Keep at modest weight (~20%) || **PM Odds Velocity** | Rapid probability changes = market attention events | Novel real-time sentiment signal |## Formal Outcome (Prediction Market Utility)**Outcome B: Discovered interesting use cases** — BTC-specific Polymarket sentiment,when filtered to Bitcoin-related markets and computed as a rolling percentile score,provides a **non-zero but modest** predictive signal. Odds velocity — how rapidlyprobabilities shift in BTC-related markets — offers an additional dimension ofsentiment information. The value of prediction market data is primarily as a**complementary indicator** rather than a standalone feature. Category-leveldecomposition reveals that Crypto-category market activity has the strongest(though still modest) correlation with BTC returns.## Time-Series Diagnostics- **Volatility clustering** (strong ACF in |returns|) justifies regime-based features- **Rolling correlation analysis** reveals whether feature relationships are stable  or require adaptive model weights## Next Steps1. Implement MVRV zone-based signal with asymmetric extreme boost in model2. Add NUPL zones as regime context overlay3. Add net exchange flow as accumulation/distribution signal4. Include hash ribbon as structural buy/sell signal5. Test fee market z-score and volume turnover as momentum confirmation6. Include halving cycle phase as structural context7. Keep Polymarket sentiment at modest weight (~20%)8. Run full backtesting with combined signal set9. Monitor rolling correlation stability to assess model robustness
